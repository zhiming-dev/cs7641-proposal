<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Group 7 Project Final Report</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="./styles.css">
    <style>
        ul {
            list-style-type: none;
        }
        li {
            text-align: justify;
            padding: 0 1;
        }
    </style>
</head>
<body>
    <div class="container my-4">
        <div class="text-center mb-4">
            <h1 class="display-4">Group 7 Project Final Report</h1>
        </div>

        <section>
            <h2 class="mb-3">Group</h2>
            <table class="table table-bordered">
                <thead class="thead-dark">
                    <tr>
                        <th>Name</th>
                        <th>Proposal Contributions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>Yuval Mazor</b></td>
                        <td>Methods, Problem Definition,Model Construction</td>
                    </tr>
                    <tr>
                        <td><b>Ruohan Feng</b></td>
                        <td>Background and Data Description, Data preprocessing</td>
                    </tr>
                    <tr>
                        <td><b>Jianwei Jia</b></td>
                        <td>Potential Results and Discussion, Data preprocessing</td>
                    </tr>
                    <tr>
                        <td><b>Wei-Hsing Huang</b></td>
                        <td>Methods, Data Description, Model Construction</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Problem Definition</h2>
            <p>
                The following study investigates how prior knowledge and expectations influence perceptual judgments in human subjects during a discrimination task. The researchers assume prior knowledge influences perception by imposing contextual constraints on sensory inputs, which enhances the speed and accuracy of detecting stimuli (Dunovan & Wheeler, 2018). This is observed in fMRI studies on category-selective regions of the inferior temporal cortex (Tremel & Wheeler, 2015). Our aim in the current study is to use machine learning algorithms to observe if prior knowledge influences people’s decision-making in response to subsequent stimuli separately for hourse and face image.
            </p>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Background and Literature Review:</h2>
            <p> The data are from a study developed by Dunovan & Wheeler (2018), which investigates the same research question but with a different approach. Previous research found indirect evidence for top-down predictions in the visual cortex, demonstrating the absence of an anticipated stimulus triggered a stronger response than seeing the anticipated stimulus itself (Kok et al., 2014). However, other studies found expected faces elicited a larger stimulus-evoked response than unexpected ones (Bell et al., 2016; Tremel et al., 2015). Thus, the current study would follow the same research questions as the previous articles and investigates if the prior expectations could enhance the response to anticipated stimuli.
                
            </p>
            <div class="text-center">
                <p class="font-weight-bold">Table 1</p>
                <p class="font-italic">Experimental Design from Dunovan & Wheeler (2018). Each trial condition is depicted along with the breakdown of the cues in each trial.</p>
                <img src="./public/1.png" class="img-fluid" alt="Experimental Design from Dunovan & Wheeler (2018)">
            </div>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Dataset Description</h2>
            <p>
                19 participants completed 600 trials (five runs of 120 trials); each run resulted in 787 medical 2D images which were converted into 3D datasets. The AFNI (Analysis of Functional Neuroimages) data is composed of two files (per trial per participant) containing the voxel numerical values, spatial characteristics of each voxel, and statistical information for each sub-brick. We will merge the files into the NIFTI file which encapsulates both metadata and the actual image data as the final dataset for machine learning in Python.
            </p>
            <div class="text-center">
                <p class="font-weight-bold">Figure 1</p>
                <p class="font-italic">Experimental Design from Dunovan & Wheeler (2018). Each trial condition is depicted along with the breakdown of the cues in each trial.</p>
                <img src="./public/2.jpg" class="img-fluid" alt="Experimental Design from Dunovan & Wheeler (2018)">
            </div>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Methods</h2>
            <li><b>Data Preprocessing:<br><br></b>
                <ul>
                    <li>To process fMRI data using AFNI, we first convert DICOM files from each run (run1 to run5) into AFNI’s BRIK/HEAD format using the to3d command, specifying the slice timing information with a scan time of 1500 ms, 785 images, and 29 slices. Next, the datasets are deobliqued using 3dWarp to standardize their orientation. Deobliquing is necessary to correct any oblique acquisition angles, ensuring that the data aligns with standard anatomical planes, which facilitates accurate processing and analysis. Then, a mask (fusiform parahippocampal mask) is resampled to match the images using 3dresample, ensuring that the mask and functional images are in the same space for accurate application. After that, we apply slice timing and motion correction with the 3dvolreg command, using the -tshift -Fourier option for slice timing correction and the -base option to set the reference volume (111th image in each run) for motion correction, while saving the motion parameters in text files. Slice timing correction adjusts for differences in acquisition times across slices, motion correction reduces bias due to subject movement, and outlier detection identifies time points with abnormal signal variations. The mask is applied to the motion-corrected images with 3dcalc to focus the analysis on the regions of interest, excluding irrelevant brain regions. Finally, we apply spatial smoothing to the motion-corrected data with the 3dmerge command, using the -1blur_fwhm 8 option to specify an 8mm FWHM Gaussian blur, and transfer the smoothed datasets to NIFTI format for machine learning data processing. </b>
                    <br></br>
                </ul> </li>

            <li><b>General Linear Model :<br><br></b>
                    <ul><li>To process fMRI data using an event-based design matrix, we start by defining key parameters such as the repetition time (TR = 1.5 seconds), the number of slices (29), and the total number of volumes (3925) per run. Frame times for the fMRI scans are calculated based on these parameters. We then load event data from a CSV file (time_event.csv), which includes onset times, durations, and trial types of the events. This event information is crucial for accurately modeling the expected brain responses during the experiment. <br><br></li>
                    
                        <li>Using the nilearn.glm.first_level.make_first_level_design_matrix function, we create the design matrix with a polynomial drift model of order 3 to account for low-frequency noise and signal drifts over time. Next, we initialize a first-level GLM model with specified parameters such as the repetition time, slice time reference, and Hemodynamic Response Function (HRF) model using the FirstLevelModel class. The first-level model is then fitted to the combined fMRI image data and the processed design matrix, estimating the parameters of the general linear model that best describe the relationship between the observed fMRI signal and the experimental design.To enhance the analysis, we define a contrast matrix as an identity matrix, which serves as a simple contrast for each condition. Contrasts are used to compare different conditions or to isolate the effect of a specific condition. For each condition, we compute the contrast using the compute_contrast method of the fitted model, producing contrast maps. These maps show regions of the brain where there are statistically significant differences in activation related to the conditions being compared. </li>
                
                
                    </ul><br><br>


                <div class="text-center">
                <figure>
                    <img src="./public/fmri_image.jpg" class="img-fluid" alt="Methods - Image 1">
                <figcaption>Figure 2: Contrast map showing the activated and deactivated regions of the brain in the condition 7.</figcaption>
                </figure><br><br>
                </div>
                <div class="text-center">
                <figure>
                    <img src="./public/process_procedure.jpg" class="img-fluid" alt="Methods - Image 2">
                <figcaption>Table 2: fMRI Data Preprocessing Steps with Python Libraries and Functions.</figcaption>
                </figure>
                </div>
            <ul>
                <li><b>Unsupervised and Supervised Learning Methods proposed<br><br></b>
                    <ul>
                        <li>Unsupervised Learning Methods for Data Processing: <br><br></li>
                            <ul>
                                <li>K-Means, GMM, DBSCAN etc.<br><br></li>
                            </ul>
                        <li>Supervised learning method for predicting the results: <br><br></li>
                            <ul>
                                <li>CNN-ResNet18: ResNet18 is highly effective for analyzing fMRI data due to its deep architecture and residual connections that facilitate learning complex patterns in brain activity.<br><br></li>
                                <li>CNN-VGG11: VGG11 is useful for fMRI data analysis as its simple yet deep architecture captures detailed spatial features in brain imaging with consistent accuracy.<br><br></li>
                                <li>CNN-MobileNetV2: MobileNetV2 excels in fMRI applications by providing efficient and accurate brain activity analysis on resource-constrained devices due to its lightweight and optimized design.<br><br></li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li><b>Data Processing Method Implemented<br><br></b>
                    <ul>
                        <li><b>1. Using to3d, 3dvolreg, 3dToutcount and 3dmerge tools in FMRI to do the 1st processing step: <br><br></b>
                        <ul>
                            <li>a. To process FMRI data using AFNI, we first converted DICOM files from each run (run1 to run5) into AFNI’s BRIK/HEAD format using the to3d command, 
                                specifying the slice timing information with the time of each scan = 1500 ms, number of images = 785, slice numbers = 29. 
                                Next, we applied slice timing and motion correction with the 3dvolreg command, 
                                using the -tshift -Fourier option for slice timing correction and the -base option to set the reference volume (111th image in each run) for motion correction, 
                                while saving the motion parameters in text files. 
                                The 3dToutcount command then computed outlier counts for each volume in the motion-corrected data, 
                                and we used -automask to create a brain mask and -fraction to output the fraction of voxels in the mask that are outliers. 
                                Finally, we applied spatial smoothing to the motion-corrected data with the 3dmerge command, 
                                used the -1blur_fwhm 4 option to specify a 4mm FWHM Gaussian blur, and transfered the smoothed datasets to NIFTI file for ML data processing.<br><br></li>
                            
                                <li>b. Slice Timing and Motion Correction: Next, we applied slice timing and motion correction with the 3dvolreg command, using the -tshift -Fourier option for slice timing correction and the -base option to set the reference volume (111th image in each run) for motion correction, while saving the motion parameters in text files.<br><br></li>
                                
                                <li>c. Musk implemenation for brain area selection: A mask (fusiform parahippocampal mask) is resampled to match the images using 3dresample, ensuring that the mask and functional images are in the same space for accurate application.</li><br><br>
                            
                                <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/s8_preproc.sh (Midterm)<br><br></li>
                                <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/s13_preproc.sh (Final Project)<br><br></li>
                                <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/fmri_data_pre.ipynb (Final Project)<br><br></li>
                            <li><b>Part of the 3D images data visualization after processing:</b> </li>
                            <li><img src="./public/1st_steps.png" alt="Part of the 3D images data visualization"></li>

                        </ul>
                        <li><b>2. Using K-means to do the 2nd processing step (in Midterm Report): <br><br></b>
                            <ul>
                                <li>K-means method:<br><br></li>
                                <ul>
                                    <li>In total we had 64x64x29x785 data points to analyze from the images, (64x64 is for one grayscale picture, for every data point, we have 29 different these images to construct a 3D brain image, and there are total 785 data points). We used K-means to remove the background noise from these 3D brain images to get a more accurate brain region recognition for model training. Our logic was that because k-means can be used to cluster the different parts of brain, if we give it a number of clusters it can be used to remove some noise points.
                                        First, 50 clusters were initiated and we used the K-means algorithm to recognize important structures. Second, we chose the 40% nearest points in a fixed distance region with the K clusters central points as our clusters and classified the
                                        other points which haven't been allocated into these clusters as noise points (similar to DBSCAN). This way, we obtained a cleaner dataset and can improve our predicted results in 
                                        when training the model.
                                    <br><br></li>
                                </ul>
                                <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/project.py (Midterm)<br><br></li>
                    </ul>

                <li><b>ML Algorithms/Models Implemented<br><br></b>
                    <ul>
                        <li><b>1. Using CNN-ResNet18 as the model to train:<br><br></b>
                            
                                <ul>
                                    <li>
                                        <strong>Model Definition:</strong>
                                        <ul>
                                            <li>a. Define an CNN resnet model using PyTorch’s torchvisoin.module.</li>
                                            <li>b. Include CNN layers to process 3D MRI image data and a fully connected layer to map the CNN's hidden state to 9 output classes.</li>
                                            <li>c. Use resnet18 for the CNN layer in our training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Initialization:</strong>
                                        <ul>
                                            <li>a. Use the pretrain model for better initialization.</li>
                                            <li>b. Set the model to use GPU if available to speed up training and inference.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Data Handling:</strong>
                                        <ul>
                                            <li>a. Define a custom dataset class to handle 3D MRI data and corresponding labels, transforming them for CNN input.</li>
                                            <li>b. Clean and preprocess the data, ensuring it is in the correct format for CNN input.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training and Validation Split:</strong>
                                        <ul>
                                            <li>a. Split the dataset into training and validation sets using an 80% - 20% split ratio.</li>
                                            <li>b. Use PyTorch's DataLoader to batch and shuffle the training data, ensuring efficient data loading during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Loss Function and Optimizer:</strong>
                                        <ul>
                                            <li>a. Use CrossEntropyLoss for multi-class classification.</li>
                                            <li>b. Implement the Adam optimizer with a learning rate of 0.01 and weight decay of 0.001 to avoid the overfitting.</li>
                                            <li>c. Incorporate a learning rate scheduler to dynamically adjust the learning rate during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training Loop (using 80 epoch for training):</strong>
                                        <ul>
                                            <li>For each epoch, perform:
                                                <ul>
                                                    <li>a. A forward pass to compute predictions.</li>
                                                    <li>b. Calculate the loss using the defined loss function.</li>
                                                    <li>c. Backpropagate the loss to compute gradients.</li>
                                                    <li>d. Update the model parameters using the optimizer.</li>
                                                </ul>
                                            </li>
                                            <li>Track and print training loss and accuracy to monitor performance.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Validation Loop:</strong>
                                        <ul>
                                            <li>a. Evaluate the model on the validation set without gradient computation.</li>
                                            <li>b. Calculate and print validation loss and accuracy for each epoch.</li>
                                            <li>c. The model result evaluation will be present in results and discussion chapter.</li>
                                        </ul>
                                    </li>
                                </ul>
                                <br><br>
                    </ul>

                    <ul>
                        <li><b>2. Using CNN-VGG11 as the model to train:<br><br></b>
                            
                                <ul>
                                    <li>
                                        <strong>Model Definition:</strong>
                                        <ul>
                                            <li>a. Define a CNN VGG11 model using PyTorch’s torchvision.models.</li>
                                            <li>b. Include CNN layers to process 3D fMRI image data and a fully connected layer to map the CNN’s hidden state to 9 output classes.</li>
                                            <li>c. Use VGG11 for the CNN layer in our training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Initialization:</strong>
                                        <ul>
                                            <li>a. Use the pretrain model for better initialization.</li>
                                            <li>b. Set the model to use GPU if available to speed up training and inference.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Data Handling:</strong>
                                        <ul>
                                            <li>a. Define a custom dataset class to handle 3D MRI data and corresponding labels, transforming them for CNN input.</li>
                                            <li>b. Clean and preprocess the data, ensuring it is in the correct format for CNN input.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training and Validation Split:</strong>
                                        <ul>
                                            <li>a. Split the dataset into training and validation sets using an 80% - 20% split ratio.</li>
                                            <li>b. Use PyTorch's DataLoader to batch and shuffle the training data, ensuring efficient data loading during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Loss Function and Optimizer:</strong>
                                        <ul>
                                            <li>a. Use CrossEntropyLoss for multi-class classification.</li>
                                            <li>b. Implement the Adam optimizer with a learning rate of 0.01 and weight decay of 0.001 to avoid the overfitting.</li>
                                            <li>c. Incorporate a learning rate scheduler to dynamically adjust the learning rate during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training Loop (using 80 epoch for training):</strong>
                                        <ul>
                                            <li>For each epoch, perform:
                                                <ul>
                                                    <li>a. A forward pass to compute predictions.</li>
                                                    <li>b. Calculate the loss using the defined loss function.</li>
                                                    <li>c. Backpropagate the loss to compute gradients.</li>
                                                    <li>d. Update the model parameters using the optimizer.</li>
                                                </ul>
                                            </li>
                                            <li>Track and print training loss and accuracy to monitor performance.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Validation Loop:</strong>
                                        <ul>
                                            <li>a. Evaluate the model on the validation set without gradient computation.</li>
                                            <li>b. Calculate and print validation loss and accuracy for each epoch.</li>
                                            <li>c. The model result evaluation will be present in results and discussion chapter.</li>
                                        </ul>
                                    </li>
                                </ul>
                                <br><br>

                            <li><b>3. Using CNN-MobileNetV2 as the model to train:<br><br></b>
                        
                                <ul>
                                    <li>
                                        <strong>Model Definition:</strong>
                                        <ul>
                                            <li>a. Define a CNN MobileNetV2 model using PyTorch’s torchvision.models.</li>
                                            <li>b. Include CNN layers to process 3D fMRI image data and a fully connected layer to map the CNN’s hidden state to XX output classes.</li>
                                            <li>c. Use MobileNetV2 for the CNN layer in our training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Initialization:</strong>
                                        <ul>
                                            <li>a. Use the pretrain model for better initialization.</li>
                                            <li>b. Set the model to use GPU if available to speed up training and inference.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Data Handling:</strong>
                                        <ul>
                                            <li>a. Define a custom dataset class to handle 3D MRI data and corresponding labels, transforming them for CNN input.</li>
                                            <li>b. Clean and preprocess the data, ensuring it is in the correct format for CNN input.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training and Validation Split:</strong>
                                        <ul>
                                            <li>a. Split the dataset into training and validation sets using an 80% - 20% split ratio.</li>
                                            <li>b. Use PyTorch's DataLoader to batch and shuffle the training data, ensuring efficient data loading during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Loss Function and Optimizer:</strong>
                                        <ul>
                                            <li>a. Use CrossEntropyLoss for multi-class classification.</li>
                                            <li>b. Implement the Adam optimizer with a learning rate of 0.01 and weight decay of 0.001 to avoid the overfitting.</li>
                                            <li>c. Incorporate a learning rate scheduler to dynamically adjust the learning rate during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training Loop (using 80 epoch for training):</strong>
                                        <ul>
                                            <li>For each epoch, perform:
                                                <ul>
                                                    <li>a. A forward pass to compute predictions.</li>
                                                    <li>b. Calculate the loss using the defined loss function.</li>
                                                    <li>c. Backpropagate the loss to compute gradients.</li>
                                                    <li>d. Update the model parameters using the optimizer.</li>
                                                </ul>
                                            </li>
                                            <li>Track and print training loss and accuracy to monitor performance.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Validation Loop:</strong>
                                        <ul>
                                            <li>a. Evaluate the model on the validation set without gradient computation.</li>
                                            <li>b. Calculate and print validation loss and accuracy for each epoch.</li>
                                            <li>c. The model result evaluation will be present in results and discussion chapter.</li>
                                        </ul>
                                    </li>
                                </ul>
                                <br><br>

                            
                        <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/project_1.py (Final)<br><br></li>
                        <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/project_Hugo_ver2.py (Final including plot)<br><br></li>
                    </ul>



            </ul>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Results and Discussion</h2>
            <ul>
                <li><b>For the validation data prediction: </b><br><br></li>
                    <ul>
                        </li>
                        <li><b>1. Draw the Confusion Matrix for the three Predict Models:</b>
                            <ul><br><br>
                                <li><b>Confusion Matrix For CNN-ResNet18:</b></li>
                                <li><img src="./public/confusion_matrix_resnet.png" alt="Confusion matrix"></li>
                            </ul>
                            <ul><br><br>
                                <li><b>Confusion Matrix For CNN-VGG11:</b></li>
                                <li><img src="./public/confusion_matrix_vgg11.png" alt="Confusion matrix"></li>
                            </ul>
                            <ul><br><br>
                                <li><b>Confusion Matrix For CNN-MobileNetV2:</b></li>
                                <li><img src="./public/confusion_matrix_mobilenet.png" alt="Confusion matrix"></li>
                            </ul>
                            <li> The confusion Matrix is used to see the relationship between the predict and true result in terms of 9 different picture conditions according to the brain 3D images, which means if we can predict the pictures successfully according to the brain activation. From above, we can know the ResNet18 and MobileNetV2 has a better performance.<br><br></li>
                        </li>

                        <li><b>2. Draw the Quantitative Metrics for the three Predict Models:</b></li>
                            <ul><br><br>
                                <li><b>Quantitative Metrics For CNN-ResNet18:</b></li>
                                <li><img src="./public/quantitative_metrics_resnet.png" alt="Quantitative metrics"></li>
                                    <li><b>Accuracy = 0.85</b></li>
                                    <li><b>Recall = 0.85</b></li>
                                    <li><b>Precision = 0.95</b></li>
                                    <li><b>F1 score = 0.89</b></li>
                            </ul>
                            <ul><br><br>
                                <li><b>Quantitative Metrics For CNN-VGG11:</b></li>
                                <li><img src="./public/quantitative_metrics_vgg11.png" alt="Quantitative metrics"></li>
                                    <li><b>Accuracy = 0.28</b></li>
                                    <li><b>Recall = 0.19</b></li>
                                    <li><b>Precision = 0.13</b></li>
                                    <li><b>F1 score = 0.13</b></li>
                            </ul>
                            <ul><br><br>
                                <li><b>Quantitative Metrics For CNN-MobileNetV2:</b></li>
                                <li><img src="./public/quantitative_metrics_mobilenet.png" alt="Quantitative metrics"></li>
                                    <li><b>Accuracy = 0.83</b></li>
                                    <li><b>Recall = 0.82</b></li>
                                    <li><b>Precision = 0.94</b></li>
                                    <li><b>F1 score = 0.86</b></li>
                            </ul>
                        </li>
                    </ul>
                </li><br><br>

                <li><b>Project Goals</b>
                    <ul>
                        <li>Confirm what areas of the ITC are activated during the study. <br><br></li>
                        <li>See if we can predict what image was being viewed from the changes in brain activation<br><br></li>
                        <li>Investigate the neural signals corresponding to the subjects' expectations during the pre- and post-sensory stages of decision-making<br><br></li>
                    </ul>
                </li><br><br>
                <li><b>Expected Results and Current Results<br><br></b>
                    <ul>
                        <li><b>1.Compared with the three different models:<br><br></b>
                            <ul>
                                <li>a. Advantages and Disadvantages for CNN-ResNet18: <br><br></li>
                                <ul>
                                    <li> Advantages: The shortcut structure makes it easier to train deep networks. It has far fewer parameters than VGG11, and when paired with custom hardware accelerators, it has more potential for real-time edge applications than VGG11. <br><br></li>
                                    <li> Disadvantages: The shortcut architecture requires specialized hardware architecture to solve memory bound problems. Fortunately, due to the recent popularity of deep learning, many companies (e.g., Google, Meta, Nvidia, AMD…) and even startups have introduced numerous SOCs and ASICs suitable for shortcut operations, making the ResNet architecture very promising for edge applications. However, although the number of parameters has significantly decreased compared to VGG11, it's still too resource-intensive for extremely low-power and low-cost edge devices. Therefore, I also implement MobileNetV2 for this task.<br><br></li>
                                </ul>

                                <li>b. Advantages and Disadvantages for CNN-VGG11: <br><br></li>
                                <ul>
                                    <li> Advantages: No shortcuts required. Since shortcuts need to store previous feature maps, VGG net may have better hardware efficiency on simpler tasks for some hardware devices that are not optimized for hardware architecture, particularly those suffering from memory bottlenecks. <br><br></li>
                                    <li> Disadvantages: Significantly more parameters than ResNet18, and due to the lack of shortcut structure, it leads to the vanishing gradient problem, resulting in much lower inference accuracy compared to ResNet18.<br><br></li>
                                </ul>


                                <li>c. Advantages and Disadvantages for CNN-MobileNetV2:<br><br></li>
                                <ul>
                                    <li> Advantages: MobileNetV2 is a lightweight neural network. At a high level, it combines pointwise convolution and depthwise convolution to significantly reduce the number of parameters while only slightly reducing accuracy. <br><br></li>
                                    <li> Disadvantages: Although the number of parameters is greatly reduced, the accuracy is lower compared to ResNet18. Therefore, in fields that are very sensitive to accuracy (e.g., medical), MobileNetV2 may not be the best choice.<br><br></li>
                                </ul>
                            </ul>


                        <li><b>2. Compared the confusion matrix and quantitative metrics for the three model's prediction performance:</b><br><br></li>
                            
                            <ul>
                                <li><b>Three Models Comparation Table:</b></li>
                                <li style="list-style: none;">
                                <figure style="display: flex; justify-content: center;">
                                    <img src="./public/comparation_table.png" alt="Models Comparation Table" style="max-width: 100%;">
                                </figure>
                            </ul>
                            
                            <ul>
                            <li>a. ResNet18: achieves the best balance between accuracy (85.39%) and parameter count, making it the top-performing model among the three.<br><br></li>
                            <li>b. MobileNetV2: offers a slightly lower accuracy (83.29%) compared to ResNet18 but excels in having the lowest parameter count, making it suitable for resource-constrained environments.<br><br></li>
                            <li>c. VGG11: has the lowest accuracy (just 27.68%) and the highest number of parameters, which might not be ideal for applications with limited resources although it has the simplest structure.<br><br></li>
                            </ul>

                        <li style="color: red;"><b>3. Overall Conclusion:</b><br><br></li>                      
                            <ul>
                                <li style="color: red;">a. We have successfully using mask area to confirm what areas of the ITC are activated during the study in data processing stage.<br><br></li>
                                <li style="color: red;">b. We have successfully using ResNet18 and MobileNetV2 to predict what image was being viewed from the brain activation fMRI images (>80% Accuracy, improving a lot compared with the midterm 20%).<br><br></li>
                                <li style="color: red;">c. We also try to use time serial model to investigate the neural signals corresponding to the subjects' expectations during the pre- and post-sensory stages of decision-making, but because of the data time content is not enough, the outcome is not good, need to do further investigation.<br><br></li>
                            </ul>
                    </ul>
                </li>
                <li><b>Further to improve:<br><br></b>
                    <ul>
                        <li>1. Data process: Use advance data process techniques to increase sample diversity, helping the model learn more robust features.<br><br></li>
                           
                        <li>2. Model Adjustment: Try different model architectures or hyperparameter optimization to enhance the model's generalization ability.<br><br></li>

                        <li>3. Feature Engineering: Extract more effective features or use new pre-trained models for feature extraction.<br><br></li>
                    </ul>
                </li>
            </ul>
        </section>

        <!-- time line style code -->
        <section>
            <h2 class="mt-4 mb-3">Timeline</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">1. Prepartion and Survey <br> (Project Proposal)</h3>
                        <p class="timeline-date">May 24 - June 14, 2024</p>
                        <p>Initial meeting to discuss FMRI project data access, goals, timeline, and responsibilities.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">2. Data Collection and Initial Data Preprocess</h3>
                        <p class="timeline-date">June 15, 2024 - June 24, 2024</p>
                        <p>Collecting and organizing the FMRI data from various sources, using FMRI tools, K-means method to do the data process and clean.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">3. Initial Model Construction and Training <br> (Midpoint Report)</h3>
                        <p class="timeline-date">June 25, 2024 - July 03, 2024</p>
                        <p>Using CNN (ResNet) to do the model training and validation.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">4. Advanced Data Process, Model Construction and Training (Using Mask to do brain area selection and construct the CNN-VGG11 and MobileNetV2).</h3>
                        <p class="timeline-date">July 04, 2024 - July 14, 2024</p>
                        <p>Cleaning and preprocessing the data for analysis.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">5. Model Improvement and Evaluation (Training and using three models to do the prediction.) <br> (Final Report)</h3>
                        <p class="timeline-date">July 15, 2024 - July 23, 2024</p>
                        <p>Training machine learning models on the preprocessed data.</p>
                    </div>
                </div>
            </div>
        </section>

        </section>
            <h2 class ="section-title">Gantt Chart</h2>
            <p><a href="https://docs.google.com/spreadsheets/d/1XyOcZbfwGnvFNNa-2XOaHMNTru0NNwUR/edit?usp=sharing&ouid=111940957351122547361&rtpof=true&sd=true" class="reference-link">Gantt chart</a></p>
        
        <section>
            <h2 class="mt-4 mb-3">References</h2>
            <ul>
                <li class="ml-4">
                    Bell, A. H., Summerfield, C., Morin, E. L., Malecek, N. J. & Ungerleider, L. G. Encoding of Stimulus Probability in Macaque Inferior Temporal Cortex. <i>Curr. Biol</i>. 1–11, https://doi.org/10.1016/j.cub.2016.07.007 (2016).
                </li>
                <li class="ml-4">
                    Dunovan, K., & Wheeler, M. E. (2018). Computational and neural signatures of pre and post-sensory expectation bias in inferior temporal cortex. <i>Scientific Reports, 8</i>(1), 13256. <a href="https://doi.org/10.1038/s41598-018-31678-x">https://doi.org/10.1038/s41598-018-31678-x</a>
                </li>
                <li class="ml-4">
                    Kok, P., Failing, M. F. & de Lange, F. P. Prior Expectations Evoke Stimulus Templates in the Primary VisualCortex. J. <i>Cogn. Neurosci. 26</i>, 194–198 (2014).
                </li>
                <li class="ml-4">
                    Michel, Vincent, et al. (2011). “A Supervised Clustering Approach for Fmri-Based Inference of Brain States.” <i>Pattern Recognition</i>, Pergamon, <a href="https://www.sciencedirect.com/science/article/pii/S0031320311001439">https://www.sciencedirect.com/science/article/pii/S0031320311001439</a>
                </li>
                <li class="ml-4">
                    Mwangi, B., Tian, T. S., & Soares, J. C. (2014). A review of feature reduction techniques in neuroimaging. Retrieved from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/</a>
                </li>
                <li class="ml-4">
                    Tremel, J. J., & Wheeler, M. E. (2015). Content-specific evidence accumulation in inferior temporal cortex during perceptual decision-making. <i>NeuroImage, 109</i>, 35–49. <a href="https://doi.org/10.1016/j.neuroimage.2014.12.072">https://doi.org/10.1016/j.neuroimage.2014.12.072</a>
                </li>
            </ul>
 
        </section>
    </div>
    
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
