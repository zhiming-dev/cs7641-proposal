<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Group 7 Project Midterm Report</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="./styles.css">
    <style>
        ul {
            list-style-type: none;
        }
        li {
            text-align: justify;
            padding: 0 1;
        }
    </style>
</head>
<body>
    <div class="container my-4">
        <div class="text-center mb-4">
            <h1 class="display-4">Group 7 Project Midterm Report</h1>
        </div>

        <section>
            <h2 class="mb-3">Group</h2>
            <table class="table table-bordered">
                <thead class="thead-dark">
                    <tr>
                        <th>Name</th>
                        <th>Proposal Contributions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>Yuval Mazor</b></td>
                        <td>Methods, Problem Definition,Model Construction</td>
                    </tr>
                    <tr>
                        <td><b>Ruohan Feng</b></td>
                        <td>Background and Data Description, Data preprocessing</td>
                    </tr>
                    <tr>
                        <td><b>Jianwei Jia</b></td>
                        <td>Potential Results and Discussion, Data preprocessing</td>
                    </tr>
                    <tr>
                        <td><b>Wei-Hsing Huang</b></td>
                        <td>Methods, Data Description, Model Construction</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Problem Definition</h2>
            <p>
                The following study investigates how prior knowledge and expectations influence perceptual judgments in human subjects during a discrimination task. The researchers assume prior knowledge influences perception by imposing contextual constraints on sensory inputs, which enhances the speed and accuracy of detecting stimuli (Dunovan & Wheeler, 2018). This is observed in fMRI studies on category-selective regions of the inferior temporal cortex (Tremel & Wheeler, 2015). Our aim in the current study is to use machine learning algorithms to observe if prior knowledge influences people’s decision-making in response to subsequent stimuli separately for hourse and face image.
            </p>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Background and Literature Review:</h2>
            <p> The data are from a study developed by Dunovan & Wheeler (2018), which investigates the same research question but with a different approach. Previous research found indirect evidence for top-down predictions in the visual cortex, demonstrating the absence of an anticipated stimulus triggered a stronger response than seeing the anticipated stimulus itself (Kok et al., 2014). However, other studies found expected faces elicited a larger stimulus-evoked response than unexpected ones (Bell et al., 2016; Tremel et al., 2015). Thus, the current study would follow the same research questions as the previous articles and investigates if the prior expectations could enhance the response to anticipated stimuli.
                
            </p>
            <div class="text-center">
                <p class="font-weight-bold">Table 1</p>
                <p class="font-italic">Experimental Design from Dunovan & Wheeler (2018). Each trial condition is depicted along with the breakdown of the cues in each trial.</p>
                <img src="./public/1.png" class="img-fluid" alt="Experimental Design from Dunovan & Wheeler (2018)">
            </div>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Dataset Description</h2>
            <p>
                19 participants completed 600 trials (five runs of 120 trials); each run resulted in 787 medical 2D images which were converted into 3D datasets. The AFNI (Analysis of Functional Neuroimages) data is composed of two files (per trial per participant) containing the voxel numerical values, spatial characteristics of each voxel, and statistical information for each sub-brick. We will merge the files into the NIFTI file which encapsulates both metadata and the actual image data as the final dataset for machine learning in Python.
            </p>
            <div class="text-center">
                <p class="font-weight-bold">Figure 1</p>
                <p class="font-italic">Experimental Design from Dunovan & Wheeler (2018). Each trial condition is depicted along with the breakdown of the cues in each trial.</p>
                <img src="./public/2.jpg" class="img-fluid" alt="Experimental Design from Dunovan & Wheeler (2018)">
            </div>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Methods</h2>
            <p>
                We analyzed the fMRI imaging and NIFTI data corresponding to experimental metadata. The .HEAD files and .BRIK files were converted into a pre-processable form using AFNI and then merged into the existing NIFTI files using pandas.
            </p>
            <div class="text-center">
                <img src="./public/3.png" class="img-fluid" alt="Methods - Image 1">
            </div>
            <div class="text-center">
                <img src="./public/4.png" class="img-fluid" alt="Methods - Image 2">
            </div><br><br>
            <ul>
                <li><b>Unsupervised and Supervised Learning Methods proposed<br><br></b>
                    <ul>
                        <li>Unsupervised Learning Methods for Data Processing: <br><br></li>
                            <ul>
                                <li>K-Means, GMM, DBSCAN etc.<br><br></li>
                            </ul>
                        <li>Supervised learning method for predicting the results: <br><br></li>
                            <ul>
                                <li>RNN: RNNs are highly effective for processing and analyzing dynamic medical images for time series analysis.<br><br></li>
                                <li>Transformer: Compared to RNN, transformers use a self-attention mechanism that can more effectively handle long-range dependencies, while RNNs may lose information or face vanishing gradients in long sequences.<br><br></li>
                                <li>CNN: Even though CNNs are not the best candidate for time series analysis, they are powerful for image tasks. Therefore, we will also apply CNNs to our project and compare the performance between CNN, RNN, and transformer.<br><br></li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li><b>Data Processing Method Implemented<br><br></b>
                    <ul>
                        <li><b>1. Using to3d, 3dvolreg, 3dToutcount and 3dmerge tools in FMRI to do the 1st processing step: <br><br></b>
                        <ul>
                            <li>a. To process FMRI data using AFNI, we first converted DICOM files from each run (run1 to run5) into AFNI’s BRIK/HEAD format using the to3d command, 
                                specifying the slice timing information with the time of each scan = 1500 ms, number of images = 785, slice numbers = 29. 
                                Next, we applied slice timing and motion correction with the 3dvolreg command, 
                                using the -tshift -Fourier option for slice timing correction and the -base option to set the reference volume (111th image in each run) for motion correction, 
                                while saving the motion parameters in text files. 
                                The 3dToutcount command then computed outlier counts for each volume in the motion-corrected data, 
                                and we used -automask to create a brain mask and -fraction to output the fraction of voxels in the mask that are outliers. 
                                Finally, we applied spatial smoothing to the motion-corrected data with the 3dmerge command, 
                                used the -1blur_fwhm 4 option to specify a 4mm FWHM Gaussian blur, and transfered the smoothed datasets to NIFTI file for ML data processing.<br><br></li>
                            
                                <li>b. Slice Timing and Motion Correction: Next, we applied slice timing and motion correction with the 3dvolreg command, using the -tshift -Fourier option for slice timing correction and the -base option to set the reference volume (111th image in each run) for motion correction, while saving the motion parameters in text files.<br><br></li>
                            <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/s8_preproc.sh<br><br></li>
                            <li><b>Part of the 3D images data visualization after processing:</b> </li>
                            <li><img src="./public/1st_steps.png" alt="Part of the 3D images data visualization"></li>

                        </ul>
                        <li><b>2. Using K-means to do the 2nd processing step: <br><br></b>
                            <ul>
                                <li>K-means method:<br><br></li>
                                <ul>
                                    <li>In total we had 64x64x29x785 data points to analyze from the images, (64x64 is for one grayscale picture, for every data point, we have 29 different these images to construct a 3D brain image, and there are total 785 data points). We used K-means to remove the background noise from these 3D brain images to get a more accurate brain region recognition for model training. Our logic was that because k-means can be used to cluster the different parts of brain, if we give it a number of clusters it can be used to remove some noise points.
                                        First, 50 clusters were initiated and we used the K-means algorithm to recognize important structures. Second, we chose the 40% nearest points in a fixed distance region with the K clusters central points as our clusters and classified the
                                        other points which haven't been allocated into these clusters as noise points (similar to DBSCAN). This way, we obtained a cleaner dataset and can improve our predicted results in 
                                        when training the model.
                                    <br><br></li>
                                </ul>
                                <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/project.py<br><br></li>
                    </ul>

                <li><b>ML Algorithms/Models Implemented<br><br></b>
                    <ul>
                        <li><b>1. Using CNN ResNet as the model to train:<br><br></b>
                            
                                <ul>
                                    <li>
                                        <strong>Model Definition:</strong>
                                        <ul>
                                            <li>a. Define an CNN resnet model using PyTorch’s torchvisoin.module.</li>
                                            <li>b. Include CNN layers to process 3D MRI image data and a fully connected layer to map the CNN's hidden state to 9 output classes.</li>
                                            <li>c. Use resnet18 for the CNN layer in our training, which can be replaced with LSTM or GRU to consider sequential information for better performance later for improvement.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Initialization:</strong>
                                        <ul>
                                            <li>a. Initialize the hidden state to zeros at the start of each sequence.</li>
                                            <li>b. Set the model to use GPU if available to speed up training and inference.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Data Handling:</strong>
                                        <ul>
                                            <li>a. Define a custom dataset class to handle 3D MRI data and corresponding labels, transforming them for CNN input.</li>
                                            <li>b. Clean and preprocess the data, ensuring it is in the correct format for CNN input.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training and Validation Split:</strong>
                                        <ul>
                                            <li>a. Split the dataset into training and validation sets using an 80% - 20% split ratio.</li>
                                            <li>b. Use PyTorch's DataLoader to batch and shuffle the training data, ensuring efficient data loading during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Loss Function and Optimizer:</strong>
                                        <ul>
                                            <li>a. Use CrossEntropyLoss for multi-class classification.</li>
                                            <li>b. Implement the Adam optimizer with a learning rate of 0.01 and weight decay of 0.001 to avoid the overfitting.</li>
                                            <li>c. Incorporate a learning rate scheduler to dynamically adjust the learning rate during training.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Training Loop (using 200 epoch for training):</strong>
                                        <ul>
                                            <li>For each epoch, perform:
                                                <ul>
                                                    <li>a. A forward pass to compute predictions.</li>
                                                    <li>b. Calculate the loss using the defined loss function.</li>
                                                    <li>c. Backpropagate the loss to compute gradients.</li>
                                                    <li>d. Update the model parameters using the optimizer.</li>
                                                </ul>
                                            </li>
                                            <li>Track and print training loss and accuracy to monitor performance.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        <strong>Validation Loop:</strong>
                                        <ul>
                                            <li>a. Evaluate the model on the validation set without gradient computation.</li>
                                            <li>b. Calculate and print validation loss and accuracy for each epoch.</li>
                                            <li>c. The model result evaluation will be present in results and discussion chapter.</li>
                                        </ul>
                                    </li>
                                </ul>
                                <br><br>
                        <li style="color: red;">Code file: CS7641_fMRI_DL/data_process_code/project.py<br><br></li>
                    </ul>



            </ul>
        </section>

        <section>
            <h2 class="mt-4 mb-3">Results and Discussion</h2>
            <ul>
                <li><b>For the validation data prediction: </b><br><br></li>
                    <ul>
                        </li>
                        <li><b>1. Draw the Confusion Matrix:</b>
                            <ul>
                                <li><img src="./public/confusion_matrix.png" alt="Confusion matrix"></li>
                                <li> The confusion Matrix is used to see the relationship between the predict and true result in terms of 9 different picture conditions according to the brain 3D images, which means if we can predict the pictures successfully according to the brain activation.<br><br></li>
                            </ul></li>

                        <li><b>2. Draw the Quantitative Metrics:</b>
                            <ul>
                                <li><img src="./public/quantitative_metrics.png" alt="Quantitative metrics"></li>
                                    </ul>
                                    <li><b>Accuracy = 0.15</b></li>
                                    <li><b>Recall = 0.09</b></li>
                                    <li><b>Precision = 0.04</b></li>
                                    <li><b>F1 score = 0.06</b></li>
                            </ul>
                    </ul>
                </li>

                <li><b>Project Goals</b>
                    <ul>
                        <li>Confirm what areas of the ITC are activated during the study</li>
                        <li>See if we can predict what image was being viewed from the changes in brain activation</li>
                        <li>Investigate the neural signals corresponding to the subjects' expectations during the pre- and post-sensory stages of decision-making</li>
                    </ul>
                </li>
                <li><b>Expected Results and Current Results<br><br></b>
                    <ul>
                        <li>1. From the confusion matrix, we can see the model’s prediction performance across different categories. Key observations include:<br><br>
                            <ul>
                            <li>a. In some picture conditions, such as 0 and 1, have relatively accurate predictions with 6 and 12 correct classifications, respectively.<br><br></li>
                            <li>b. conditions 4, 5, and 6 have significant misclassifications. For instance, 14 samples of the actual category 5 are misclassified as 1, indicating the model’s poor distinction ability for these categories.<br><br></li>
                            <li>c. Some conditions, like 3 and 8, have very few correct classifications.<br><br></li>
                            <b>These classification errors might be due to overlapping features among different categories or imbalanced training data, making it difficult for the model to learn distinct features for certain categories</b><br><br></li>
                            </ul>
                        <li>2.From quantitative metrics we know the Accuracy, Recall, Precision, and F1 Score, reflecting the overall performance of the model:<br><br>
                            <ul>
                                <li>a. Accuracy (0.15): This low value indicates that the model correctly classifies only 15% of the samples, suggesting it often fails to predict the correct labels.<br><br></li>
                                <li>b. Recall (0.09): A recall of 0.09 means that only 9% of the actual positive samples are correctly identified. Low recall indicates the model’s poor ability to recognize positive samples.<br><br></li>
                                <li>c. Precision (0.04): A precision of 0.04 means that only 4% of the samples predicted as a certain category are actually of that category, indicating many false positives.<br><br></li>
                                <li>d. F1 Score (0.06): The F1 score, which balances precision and recall, is 0.06, further demonstrating the model’s poor performance.<br><br></li>
                            </ul>
                            <b>These values reveal that the model’s average performance is poor and indicate that the model needs improvement, better feature engineering, and adjustments will be done for the model architecture later, but generally the FMRI priection accuracy is low both in academic and industry, we will do the further investigate to see if these values are acceptable.</b><br><br></li>
                    </ul>
                </li>
                <li><b>Next step to improve:<br><br></b>
                    <ul>
                        <li>1. Data process: Use advance data process techniques to increase sample diversity, helping the model learn more robust features.<br><br></li>
                           
                        <li>2. Model Adjustment: Try different model architectures or hyperparameter optimization to enhance the model’s generalization ability.<br><br></li>

                        <li>3. Feature Engineering: Extract more effective features or use new pre-trained models for feature extraction.<br><br></li>
                    </ul>
                </li>
            </ul>
        </section>

        <!-- time line style code -->
        <section>
            <h2 class="mt-4 mb-3">Timeline</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">1. Prepartion and Survey <br> (Project Proposal)</h3>
                        <p class="timeline-date">May 24 - June 14, 2024</p>
                        <p>Initial meeting to discuss FMRI project data access, goals, timeline, and responsibilities.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">2. Data Collection and Initial Data Preprocess</h3>
                        <p class="timeline-date">June 15, 2024 - June 24, 2024</p>
                        <p>Collecting and organizing the FMRI data from various sources, using FMRI tools, K-means method to do the data process and clean.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title" style="color: red;">3. Initial Model Construction and Training <br> (Midpoint Report)</h3>
                        <p class="timeline-date">June 25, 2024 - July 03, 2024</p>
                        <p>Using CNN (ResNet) to do the model training and validation.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title">4. Advanced Data Process, Model Construction and Training</h3>
                        <p class="timeline-date">July 04, 2024 - July 14, 2024</p>
                        <p>Cleaning and preprocessing the data for analysis.</p>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-icon"></div>
                    <div class="timeline-content">
                        <h3 class="timeline-title">5. Model Improvement and Evaluation <br> (Final Report)</h3>
                        <p class="timeline-date">July 15, 2024 - July 23, 2024</p>
                        <p>Training machine learning models on the preprocessed data.</p>
                    </div>
                </div>
            </div>
        </section>

        </section>
            <h2 class ="section-title">Gantt Chart</h2>
            <p><a href="https://docs.google.com/spreadsheets/d/1XyOcZbfwGnvFNNa-2XOaHMNTru0NNwUR/edit?usp=sharing&ouid=111940957351122547361&rtpof=true&sd=true" class="reference-link">Gantt chart</a></p>
        
        <section>
            <h2 class="mt-4 mb-3">References</h2>
            <ul>
                <li class="ml-4">
                    Bell, A. H., Summerfield, C., Morin, E. L., Malecek, N. J. & Ungerleider, L. G. Encoding of Stimulus Probability in Macaque Inferior Temporal Cortex. <i>Curr. Biol</i>. 1–11, https://doi.org/10.1016/j.cub.2016.07.007 (2016).
                </li>
                <li class="ml-4">
                    Dunovan, K., & Wheeler, M. E. (2018). Computational and neural signatures of pre and post-sensory expectation bias in inferior temporal cortex. <i>Scientific Reports, 8</i>(1), 13256. <a href="https://doi.org/10.1038/s41598-018-31678-x">https://doi.org/10.1038/s41598-018-31678-x</a>
                </li>
                <li class="ml-4">
                    Kok, P., Failing, M. F. & de Lange, F. P. Prior Expectations Evoke Stimulus Templates in the Primary VisualCortex. J. <i>Cogn. Neurosci. 26</i>, 194–198 (2014).
                </li>
                <li class="ml-4">
                    Michel, Vincent, et al. (2011). “A Supervised Clustering Approach for Fmri-Based Inference of Brain States.” <i>Pattern Recognition</i>, Pergamon, <a href="https://www.sciencedirect.com/science/article/pii/S0031320311001439">https://www.sciencedirect.com/science/article/pii/S0031320311001439</a>
                </li>
                <li class="ml-4">
                    Mwangi, B., Tian, T. S., & Soares, J. C. (2014). A review of feature reduction techniques in neuroimaging. Retrieved from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/</a>
                </li>
                <li class="ml-4">
                    Tremel, J. J., & Wheeler, M. E. (2015). Content-specific evidence accumulation in inferior temporal cortex during perceptual decision-making. <i>NeuroImage, 109</i>, 35–49. <a href="https://doi.org/10.1016/j.neuroimage.2014.12.072">https://doi.org/10.1016/j.neuroimage.2014.12.072</a>
                </li>
            </ul>
 
        </section>
    </div>
    
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
